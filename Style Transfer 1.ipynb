{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76d57661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3b85c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to style, content, and output folders\n",
    "style_image_path = 'Style/3.jpg'\n",
    "content_image_path = 'Content/6.jpg'\n",
    "output_folder = 'Output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e63e7554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d167e7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to preprocess the input images\n",
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path)\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = tf.keras.applications.vgg19.preprocess_input(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22e3b2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to deprocess the generated image\n",
    "def deprocess_image(x):\n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "    x = x[:, :, ::-1]\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e9585a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the style and content images\n",
    "style_image = preprocess_image(style_image_path)\n",
    "content_image = preprocess_image(content_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01276225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a VGG19 model with the desired layers for feature extraction\n",
    "base_model = VGG19(weights='imagenet', include_top=False)\n",
    "style_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']\n",
    "content_layer = 'block4_conv2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f27304b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models for style and content feature extraction\n",
    "style_extractor = Model(inputs=base_model.input, outputs=[base_model.get_layer(layer).output for layer in style_layers])\n",
    "content_extractor = Model(inputs=base_model.input, outputs=base_model.get_layer(content_layer).output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "222afd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define style and content target values\n",
    "style_targets = style_extractor(style_image)\n",
    "content_target = content_extractor(content_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1dadb3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables for the generated image and set them as trainable\n",
    "generated_image = tf.Variable(content_image, dtype=tf.float32)\n",
    "optimizer = Adam(learning_rate=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee0dfc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate the style loss\n",
    "def style_loss(style_targets, style_outputs):\n",
    "    style_loss = 0\n",
    "    for target, output in zip(style_targets, style_outputs):\n",
    "        target_gram = tf.linalg.einsum('bijc,bijd->bcd', target, target)\n",
    "        output_gram = tf.linalg.einsum('bijc,bijd->bcd', output, output)\n",
    "        style_loss += tf.reduce_mean(tf.square(target_gram - output_gram))\n",
    "    return style_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45526c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate the content loss\n",
    "def content_loss(content_target, content_output):\n",
    "    return tf.reduce_mean(tf.square(content_target - content_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "565d184c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate the total variation loss\n",
    "def total_variation_loss(x):\n",
    "    a = tf.square(x[:, :-1, :-1, :] - x[:, 1:, :-1, :])\n",
    "    b = tf.square(x[:, :-1, :-1, :] - x[:, :-1, 1:, :])\n",
    "    return tf.reduce_sum(tf.pow(a + b, 1.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd7ab8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the total loss as a combination of style, content, and total variation loss\n",
    "total_variation_weight = 1e-6\n",
    "style_weight = 1e-2\n",
    "content_weight = 1e4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0ceb498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the total loss for the style transfer model\n",
    "def total_loss(generated_image, style_targets, content_target):\n",
    "    # Extract style features from the generated image using the style extractor model\n",
    "    style_outputs = style_extractor(generated_image)\n",
    "    \n",
    "    # Extract content features from the generated image using the content extractor model\n",
    "    content_output = content_extractor(generated_image)\n",
    "    \n",
    "    # Compute the total loss as a combination of style loss, content loss, and total variation loss\n",
    "    # The weights (style_weight, content_weight, total_variation_weight) control the contribution of each loss component\n",
    "    \n",
    "    # Style loss measures the difference between the style of the generated image and the target style\n",
    "    style_loss_value = style_loss(style_targets, style_outputs)\n",
    "    \n",
    "    # Content loss measures the difference between the content of the generated image and the target content\n",
    "    content_loss_value = content_loss(content_target, content_output)\n",
    "    \n",
    "    # Total variation loss encourages spatial smoothness in the generated image\n",
    "    tv_loss_value = total_variation_loss(generated_image)\n",
    "    \n",
    "    # Combine the individual losses with their respective weights to get the total loss\n",
    "    total_loss_value = (\n",
    "        style_weight * style_loss_value +\n",
    "        content_weight * content_loss_value +\n",
    "        total_variation_weight * tv_loss_value\n",
    "    )\n",
    "    \n",
    "    return total_loss_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d01e838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to train the style transfer model and save images after specified epochs\n",
    "def train(style_targets, content_target, generated_image, epochs, save_interval):\n",
    "    # Initialize a step counter\n",
    "    step = 0\n",
    "    \n",
    "    # Iterate through the specified number of epochs\n",
    "    for epoch in range(epochs):\n",
    "        # Use a gradient tape to record operations for automatic differentiation\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Calculate the total loss for the current generated image\n",
    "            loss = total_loss(generated_image, style_targets, content_target)\n",
    "        \n",
    "        # Compute the gradients of the loss with respect to the generated image\n",
    "        grads = tape.gradient(loss, generated_image)\n",
    "        \n",
    "        # Update the generated image using the optimizer and computed gradients\n",
    "        optimizer.apply_gradients([(grads, generated_image)])\n",
    "        \n",
    "        # Check if the current epoch is a multiple of the specified save interval\n",
    "        if (epoch + 1) % save_interval == 0:\n",
    "            # Save the generated image\n",
    "            generated_img_np = deprocess_image(generated_image.numpy()[0])\n",
    "            save_path = os.path.join(output_folder, f'{os.path.splitext(os.path.basename(content_image_path))[0]}_{epoch + 1}.jpg')\n",
    "            tf.keras.preprocessing.image.save_img(save_path, generated_img_np)\n",
    "\n",
    "        # Print the training progress (epoch, total loss)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss}\")\n",
    "        \n",
    "        # Increment the step counter\n",
    "        step += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c036dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of training epochs\n",
    "epochs = 300\n",
    "\n",
    "# Set the interval at which to save the generated image during training\n",
    "save_interval = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04ae4f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Loss: 1.6628513580693586e+18\n",
      "Epoch 2/1000, Loss: 1.6156413524296335e+18\n",
      "Epoch 3/1000, Loss: 1.563843634523013e+18\n",
      "Epoch 4/1000, Loss: 1.5097512358492242e+18\n",
      "Epoch 5/1000, Loss: 1.454097118347133e+18\n",
      "Epoch 6/1000, Loss: 1.396679659071996e+18\n",
      "Epoch 7/1000, Loss: 1.337596989436592e+18\n",
      "Epoch 8/1000, Loss: 1.277154361356583e+18\n",
      "Epoch 9/1000, Loss: 1.2157169501313434e+18\n",
      "Epoch 10/1000, Loss: 1.1537255224846582e+18\n",
      "Epoch 11/1000, Loss: 1.0917066757667553e+18\n",
      "Epoch 12/1000, Loss: 1.0301960095793152e+18\n",
      "Epoch 13/1000, Loss: 9.697096759121019e+17\n",
      "Epoch 14/1000, Loss: 9.106813633827963e+17\n",
      "Epoch 15/1000, Loss: 8.534830505189704e+17\n",
      "Epoch 16/1000, Loss: 7.9840136625809e+17\n",
      "Epoch 17/1000, Loss: 7.456549759751291e+17\n",
      "Epoch 18/1000, Loss: 6.95381249904083e+17\n",
      "Epoch 19/1000, Loss: 6.476639570870927e+17\n",
      "Epoch 20/1000, Loss: 6.02544604088107e+17\n",
      "Epoch 21/1000, Loss: 5.6004930430828544e+17\n",
      "Epoch 22/1000, Loss: 5.201973335608525e+17\n",
      "Epoch 23/1000, Loss: 4.83018000702636e+17\n",
      "Epoch 24/1000, Loss: 4.48547898854998e+17\n",
      "Epoch 25/1000, Loss: 4.168379835099382e+17\n",
      "Epoch 26/1000, Loss: 3.879105884973957e+17\n",
      "Epoch 27/1000, Loss: 3.6174784675341926e+17\n",
      "Epoch 28/1000, Loss: 3.382736858172621e+17\n",
      "Epoch 29/1000, Loss: 3.173605279803638e+17\n",
      "Epoch 30/1000, Loss: 2.98845782960767e+17\n",
      "Epoch 31/1000, Loss: 2.8251451341511066e+17\n",
      "Epoch 32/1000, Loss: 2.681082825712599e+17\n",
      "Epoch 33/1000, Loss: 2.5537590356187546e+17\n",
      "Epoch 34/1000, Loss: 2.4401356758030746e+17\n",
      "Epoch 35/1000, Loss: 2.3376125730645606e+17\n",
      "Epoch 36/1000, Loss: 2.2439494724616192e+17\n",
      "Epoch 37/1000, Loss: 2.157123787995218e+17\n",
      "Epoch 38/1000, Loss: 2.075772812441682e+17\n",
      "Epoch 39/1000, Loss: 1.998709073243013e+17\n",
      "Epoch 40/1000, Loss: 1.9250309708644352e+17\n",
      "Epoch 41/1000, Loss: 1.8542504252224307e+17\n",
      "Epoch 42/1000, Loss: 1.7861605906920243e+17\n",
      "Epoch 43/1000, Loss: 1.720810773497774e+17\n",
      "Epoch 44/1000, Loss: 1.6583289636651008e+17\n",
      "Epoch 45/1000, Loss: 1.5987880038393446e+17\n",
      "Epoch 46/1000, Loss: 1.542282039703634e+17\n",
      "Epoch 47/1000, Loss: 1.4889134632783053e+17\n",
      "Epoch 48/1000, Loss: 1.4387305499957658e+17\n",
      "Epoch 49/1000, Loss: 1.3917354473396634e+17\n",
      "Epoch 50/1000, Loss: 1.3479254065309286e+17\n",
      "Epoch 51/1000, Loss: 1.307232309388247e+17\n",
      "Epoch 52/1000, Loss: 1.2694696684316262e+17\n",
      "Epoch 53/1000, Loss: 1.2344430075419034e+17\n",
      "Epoch 54/1000, Loss: 1.2019451374967194e+17\n",
      "Epoch 55/1000, Loss: 1.171766893388759e+17\n",
      "Epoch 56/1000, Loss: 1.1436302190352794e+17\n",
      "Epoch 57/1000, Loss: 1.1172378168000512e+17\n",
      "Epoch 58/1000, Loss: 1.092374508821545e+17\n",
      "Epoch 59/1000, Loss: 1.0688793197255066e+17\n",
      "Epoch 60/1000, Loss: 1.0465786469338317e+17\n",
      "Epoch 61/1000, Loss: 1.0252995750631834e+17\n",
      "Epoch 62/1000, Loss: 1.0048909212647424e+17\n",
      "Epoch 63/1000, Loss: 9.853488200679424e+16\n",
      "Epoch 64/1000, Loss: 9.666063558823117e+16\n",
      "Epoch 65/1000, Loss: 9.485704997162189e+16\n",
      "Epoch 66/1000, Loss: 9.312086098182144e+16\n",
      "Epoch 67/1000, Loss: 9.145049666079949e+16\n",
      "Epoch 68/1000, Loss: 8.984618893679002e+16\n",
      "Epoch 69/1000, Loss: 8.830767152182067e+16\n",
      "Epoch 70/1000, Loss: 8.682894864154624e+16\n",
      "Epoch 71/1000, Loss: 8.540870603597414e+16\n",
      "Epoch 72/1000, Loss: 8.404302669493043e+16\n",
      "Epoch 73/1000, Loss: 8.273162715057357e+16\n",
      "Epoch 74/1000, Loss: 8.14719819621335e+16\n",
      "Epoch 75/1000, Loss: 8.02614540196905e+16\n",
      "Epoch 76/1000, Loss: 7.90970883857449e+16\n",
      "Epoch 77/1000, Loss: 7.797564665495552e+16\n",
      "Epoch 78/1000, Loss: 7.689401068106547e+16\n",
      "Epoch 79/1000, Loss: 7.584974951258522e+16\n",
      "Epoch 80/1000, Loss: 7.484132555122278e+16\n",
      "Epoch 81/1000, Loss: 7.38667717019566e+16\n",
      "Epoch 82/1000, Loss: 7.292497127328973e+16\n",
      "Epoch 83/1000, Loss: 7.201505668182835e+16\n",
      "Epoch 84/1000, Loss: 7.113515961679872e+16\n",
      "Epoch 85/1000, Loss: 7.028199442821939e+16\n",
      "Epoch 86/1000, Loss: 6.945612375680614e+16\n",
      "Epoch 87/1000, Loss: 6.865715246556774e+16\n",
      "Epoch 88/1000, Loss: 6.788321224373043e+16\n",
      "Epoch 89/1000, Loss: 6.713236606104371e+16\n",
      "Epoch 90/1000, Loss: 6.640332542731878e+16\n",
      "Epoch 91/1000, Loss: 6.569597437843866e+16\n",
      "Epoch 92/1000, Loss: 6.500942815114035e+16\n",
      "Epoch 93/1000, Loss: 6.434211478739354e+16\n",
      "Epoch 94/1000, Loss: 6.369320535851008e+16\n",
      "Epoch 95/1000, Loss: 6.306148438874522e+16\n",
      "Epoch 96/1000, Loss: 6.244700771267379e+16\n",
      "Epoch 97/1000, Loss: 6.184889915696742e+16\n",
      "Epoch 98/1000, Loss: 6.126617946908262e+16\n",
      "Epoch 99/1000, Loss: 6.069831607307469e+16\n",
      "Epoch 100/1000, Loss: 6.014498684639642e+16\n",
      "Epoch 101/1000, Loss: 5.96054444647383e+16\n",
      "Epoch 102/1000, Loss: 5.90792121867305e+16\n",
      "Epoch 103/1000, Loss: 5.856530216989491e+16\n",
      "Epoch 104/1000, Loss: 5.806347389606298e+16\n",
      "Epoch 105/1000, Loss: 5.757310888994406e+16\n",
      "Epoch 106/1000, Loss: 5.70937475900375e+16\n",
      "Epoch 107/1000, Loss: 5.66250592838615e+16\n",
      "Epoch 108/1000, Loss: 5.616677768344371e+16\n",
      "Epoch 109/1000, Loss: 5.571845181721805e+16\n",
      "Epoch 110/1000, Loss: 5.528006021034803e+16\n",
      "Epoch 111/1000, Loss: 5.485085124355686e+16\n",
      "Epoch 112/1000, Loss: 5.443049849933005e+16\n",
      "Epoch 113/1000, Loss: 5.401888601355059e+16\n",
      "Epoch 114/1000, Loss: 5.361542537569894e+16\n",
      "Epoch 115/1000, Loss: 5.322006075120026e+16\n",
      "Epoch 116/1000, Loss: 5.283254303195136e+16\n",
      "Epoch 117/1000, Loss: 5.245262310984909e+16\n",
      "Epoch 118/1000, Loss: 5.20800261069865e+16\n",
      "Epoch 119/1000, Loss: 5.171452868506419e+16\n",
      "Epoch 120/1000, Loss: 5.13557872466985e+16\n",
      "Epoch 121/1000, Loss: 5.100364717306675e+16\n",
      "Epoch 122/1000, Loss: 5.065807410443059e+16\n",
      "Epoch 123/1000, Loss: 5.031874591824282e+16\n",
      "Epoch 124/1000, Loss: 4.998523741274112e+16\n",
      "Epoch 125/1000, Loss: 4.965750993321984e+16\n",
      "Epoch 126/1000, Loss: 4.93354346306601e+16\n",
      "Epoch 127/1000, Loss: 4.901887406610842e+16\n",
      "Epoch 128/1000, Loss: 4.870775093015347e+16\n",
      "Epoch 129/1000, Loss: 4.84019148989399e+16\n",
      "Epoch 130/1000, Loss: 4.810104814488781e+16\n",
      "Epoch 131/1000, Loss: 4.780508194852045e+16\n",
      "Epoch 132/1000, Loss: 4.751401630983782e+16\n",
      "Epoch 133/1000, Loss: 4.722764936537702e+16\n",
      "Epoch 134/1000, Loss: 4.694570194226381e+16\n",
      "Epoch 135/1000, Loss: 4.666826852977869e+16\n",
      "Epoch 136/1000, Loss: 4.639510431478579e+16\n",
      "Epoch 137/1000, Loss: 4.612607185833165e+16\n",
      "Epoch 138/1000, Loss: 4.586110673590682e+16\n",
      "Epoch 139/1000, Loss: 4.560013593306726e+16\n",
      "Epoch 140/1000, Loss: 4.53429404064809e+16\n",
      "Epoch 141/1000, Loss: 4.508935265242317e+16\n",
      "Epoch 142/1000, Loss: 4.483947145514189e+16\n",
      "Epoch 143/1000, Loss: 4.459322380019302e+16\n",
      "Epoch 144/1000, Loss: 4.435044218385203e+16\n",
      "Epoch 145/1000, Loss: 4.411096339736166e+16\n",
      "Epoch 146/1000, Loss: 4.38748089155584e+16\n",
      "Epoch 147/1000, Loss: 4.364192290386739e+16\n",
      "Epoch 148/1000, Loss: 4.341224952771379e+16\n",
      "Epoch 149/1000, Loss: 4.318563416827494e+16\n",
      "Epoch 150/1000, Loss: 4.296198233627034e+16\n",
      "Epoch 151/1000, Loss: 4.274133268640563e+16\n",
      "Epoch 152/1000, Loss: 4.25235005350871e+16\n",
      "Epoch 153/1000, Loss: 4.230855030682419e+16\n",
      "Epoch 154/1000, Loss: 4.209639180730368e+16\n",
      "Epoch 155/1000, Loss: 4.188690048247398e+16\n",
      "Epoch 156/1000, Loss: 4.168011928200806e+16\n",
      "Epoch 157/1000, Loss: 4.147593653675622e+16\n",
      "Epoch 158/1000, Loss: 4.12743178869801e+16\n",
      "Epoch 159/1000, Loss: 4.107519031823565e+16\n",
      "Epoch 160/1000, Loss: 4.087859248522854e+16\n",
      "Epoch 161/1000, Loss: 4.06845029131223e+16\n",
      "Epoch 162/1000, Loss: 4.04928399975383e+16\n",
      "Epoch 163/1000, Loss: 4.030346200455578e+16\n",
      "Epoch 164/1000, Loss: 4.011630450966528e+16\n",
      "Epoch 165/1000, Loss: 3.993136321789952e+16\n",
      "Epoch 166/1000, Loss: 3.974861235945472e+16\n",
      "Epoch 167/1000, Loss: 3.956806052426547e+16\n",
      "Epoch 168/1000, Loss: 3.938956168344371e+16\n",
      "Epoch 169/1000, Loss: 3.921314590176051e+16\n",
      "Epoch 170/1000, Loss: 3.90387831144448e+16\n",
      "Epoch 171/1000, Loss: 3.886636594731418e+16\n",
      "Epoch 172/1000, Loss: 3.86959330550743e+16\n",
      "Epoch 173/1000, Loss: 3.852742001321574e+16\n",
      "Epoch 174/1000, Loss: 3.836073662742528e+16\n",
      "Epoch 175/1000, Loss: 3.81959000775721e+16\n",
      "Epoch 176/1000, Loss: 3.80329060686889e+16\n",
      "Epoch 177/1000, Loss: 3.787164722659328e+16\n",
      "Epoch 178/1000, Loss: 3.771208489657958e+16\n",
      "Epoch 179/1000, Loss: 3.755421048871322e+16\n",
      "Epoch 180/1000, Loss: 3.739804118286336e+16\n",
      "Epoch 181/1000, Loss: 3.724351684948787e+16\n",
      "Epoch 182/1000, Loss: 3.709060742381568e+16\n",
      "Epoch 183/1000, Loss: 3.693921412159898e+16\n",
      "Epoch 184/1000, Loss: 3.678933264787046e+16\n",
      "Epoch 185/1000, Loss: 3.664097159256474e+16\n",
      "Epoch 186/1000, Loss: 3.649410518587802e+16\n",
      "Epoch 187/1000, Loss: 3.634866900330086e+16\n",
      "Epoch 188/1000, Loss: 3.620468451966976e+16\n",
      "Epoch 189/1000, Loss: 3.606209590040986e+16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 190/1000, Loss: 3.592096757003059e+16\n",
      "Epoch 191/1000, Loss: 3.5781153499643904e+16\n",
      "Epoch 192/1000, Loss: 3.5642673016602624e+16\n",
      "Epoch 193/1000, Loss: 3.550548746620109e+16\n",
      "Epoch 194/1000, Loss: 3.5369618323275776e+16\n",
      "Epoch 195/1000, Loss: 3.5235046260473856e+16\n",
      "Epoch 196/1000, Loss: 3.5101711148253184e+16\n",
      "Epoch 197/1000, Loss: 3.4969585069326336e+16\n",
      "Epoch 198/1000, Loss: 3.4838676613627904e+16\n",
      "Epoch 199/1000, Loss: 3.4708914914197504e+16\n",
      "Epoch 200/1000, Loss: 3.458038157541376e+16\n",
      "Epoch 201/1000, Loss: 3.4453042237538304e+16\n",
      "Epoch 202/1000, Loss: 3.4326864688316416e+16\n",
      "Epoch 203/1000, Loss: 3.420179953562419e+16\n",
      "Epoch 204/1000, Loss: 3.4077801682305024e+16\n",
      "Epoch 205/1000, Loss: 3.395493984783565e+16\n",
      "Epoch 206/1000, Loss: 3.383319470486323e+16\n",
      "Epoch 207/1000, Loss: 3.371250827132928e+16\n",
      "Epoch 208/1000, Loss: 3.359286336736461e+16\n",
      "Epoch 209/1000, Loss: 3.3474229928198144e+16\n",
      "Epoch 210/1000, Loss: 3.335656070918963e+16\n",
      "Epoch 211/1000, Loss: 3.3239851415371776e+16\n",
      "Epoch 212/1000, Loss: 3.312410848919552e+16\n",
      "Epoch 213/1000, Loss: 3.300935555298099e+16\n",
      "Epoch 214/1000, Loss: 3.2895579721826304e+16\n",
      "Epoch 215/1000, Loss: 3.2782748783476736e+16\n",
      "Epoch 216/1000, Loss: 3.267085844296499e+16\n",
      "Epoch 217/1000, Loss: 3.255987648803635e+16\n",
      "Epoch 218/1000, Loss: 3.2449815803592704e+16\n",
      "Epoch 219/1000, Loss: 3.234070430692147e+16\n",
      "Epoch 220/1000, Loss: 3.223247542602957e+16\n",
      "Epoch 221/1000, Loss: 3.212515278323712e+16\n",
      "Epoch 222/1000, Loss: 3.201868269145293e+16\n",
      "Epoch 223/1000, Loss: 3.191303508590592e+16\n",
      "Epoch 224/1000, Loss: 3.180818204930867e+16\n",
      "Epoch 225/1000, Loss: 3.1704123581661184e+16\n",
      "Epoch 226/1000, Loss: 3.160087471534899e+16\n",
      "Epoch 227/1000, Loss: 3.1498413975535616e+16\n",
      "Epoch 228/1000, Loss: 3.1396732772286464e+16\n",
      "Epoch 229/1000, Loss: 3.1295833253085184e+16\n",
      "Epoch 230/1000, Loss: 3.1195640256004096e+16\n",
      "Epoch 231/1000, Loss: 3.1096211763101696e+16\n",
      "Epoch 232/1000, Loss: 3.099753918444339e+16\n",
      "Epoch 233/1000, Loss: 3.089959245525811e+16\n",
      "Epoch 234/1000, Loss: 3.080235654316032e+16\n",
      "Epoch 235/1000, Loss: 3.0705863660404736e+16\n",
      "Epoch 236/1000, Loss: 3.061006226738381e+16\n",
      "Epoch 237/1000, Loss: 3.051498242886861e+16\n",
      "Epoch 238/1000, Loss: 3.0420591932604416e+16\n",
      "Epoch 239/1000, Loss: 3.0326920843362304e+16\n",
      "Epoch 240/1000, Loss: 3.023391762153472e+16\n",
      "Epoch 241/1000, Loss: 3.0141582267121664e+16\n",
      "Epoch 242/1000, Loss: 3.004989115780301e+16\n",
      "Epoch 243/1000, Loss: 2.995882281874227e+16\n",
      "Epoch 244/1000, Loss: 2.9868325710331904e+16\n",
      "Epoch 245/1000, Loss: 2.977846640456499e+16\n",
      "Epoch 246/1000, Loss: 2.9689204099252224e+16\n",
      "Epoch 247/1000, Loss: 2.960057100664832e+16\n",
      "Epoch 248/1000, Loss: 2.951251343966208e+16\n",
      "Epoch 249/1000, Loss: 2.942502495584256e+16\n",
      "Epoch 250/1000, Loss: 2.93380626055168e+16\n",
      "Epoch 251/1000, Loss: 2.9251662895906816e+16\n",
      "Epoch 252/1000, Loss: 2.9165830121979904e+16\n",
      "Epoch 253/1000, Loss: 2.908054495638323e+16\n",
      "Epoch 254/1000, Loss: 2.899581813653504e+16\n",
      "Epoch 255/1000, Loss: 2.8911630335082496e+16\n",
      "Epoch 256/1000, Loss: 2.882799228944384e+16\n",
      "Epoch 257/1000, Loss: 2.874489326220083e+16\n",
      "Epoch 258/1000, Loss: 2.866232251593523e+16\n",
      "Epoch 259/1000, Loss: 2.858027790316339e+16\n",
      "Epoch 260/1000, Loss: 2.849875942388531e+16\n",
      "Epoch 261/1000, Loss: 2.841776063565005e+16\n",
      "Epoch 262/1000, Loss: 2.833726221110477e+16\n",
      "Epoch 263/1000, Loss: 2.825725556031488e+16\n",
      "Epoch 264/1000, Loss: 2.8177717060960256e+16\n",
      "Epoch 265/1000, Loss: 2.809865530297549e+16\n",
      "Epoch 266/1000, Loss: 2.802004666404045e+16\n",
      "Epoch 267/1000, Loss: 2.794190832402432e+16\n",
      "Epoch 268/1000, Loss: 2.786423598795981e+16\n",
      "Epoch 269/1000, Loss: 2.778700818101043e+16\n",
      "Epoch 270/1000, Loss: 2.7710252820463616e+16\n",
      "Epoch 271/1000, Loss: 2.763390762929357e+16\n",
      "Epoch 272/1000, Loss: 2.7557996229820416e+16\n",
      "Epoch 273/1000, Loss: 2.748251862204416e+16\n",
      "Epoch 274/1000, Loss: 2.740750487073587e+16\n",
      "Epoch 275/1000, Loss: 2.7332886256418816e+16\n",
      "Epoch 276/1000, Loss: 2.7258679958962176e+16\n",
      "Epoch 277/1000, Loss: 2.7184873093464064e+16\n",
      "Epoch 278/1000, Loss: 2.7111461364957184e+16\n",
      "Epoch 279/1000, Loss: 2.703847054324531e+16\n",
      "Epoch 280/1000, Loss: 2.696591995568128e+16\n",
      "Epoch 281/1000, Loss: 2.6893770947559424e+16\n",
      "Epoch 282/1000, Loss: 2.682204714119987e+16\n",
      "Epoch 283/1000, Loss: 2.675071847183155e+16\n",
      "Epoch 284/1000, Loss: 2.667976775958528e+16\n",
      "Epoch 285/1000, Loss: 2.660923365916672e+16\n",
      "Epoch 286/1000, Loss: 2.6539092548255744e+16\n",
      "Epoch 287/1000, Loss: 2.6469320804532224e+16\n",
      "Epoch 288/1000, Loss: 2.639992057547981e+16\n",
      "Epoch 289/1000, Loss: 2.633091763090227e+16\n",
      "Epoch 290/1000, Loss: 2.62622754635776e+16\n",
      "Epoch 291/1000, Loss: 2.619400695840768e+16\n",
      "Epoch 292/1000, Loss: 2.6126088493072384e+16\n",
      "Epoch 293/1000, Loss: 2.605855227982643e+16\n",
      "Epoch 294/1000, Loss: 2.599135966396416e+16\n",
      "Epoch 295/1000, Loss: 2.5924514940452864e+16\n",
      "Epoch 296/1000, Loss: 2.585802455174349e+16\n",
      "Epoch 297/1000, Loss: 2.5791873465450496e+16\n",
      "Epoch 298/1000, Loss: 2.572605738660659e+16\n",
      "Epoch 299/1000, Loss: 2.5660578462695424e+16\n",
      "Epoch 300/1000, Loss: 2.5595434546233344e+16\n",
      "Epoch 301/1000, Loss: 2.553060845735117e+16\n",
      "Epoch 302/1000, Loss: 2.546610663849984e+16\n",
      "Epoch 303/1000, Loss: 2.540197203935232e+16\n",
      "Epoch 304/1000, Loss: 2.5338187480039424e+16\n",
      "Epoch 305/1000, Loss: 2.527474222314291e+16\n",
      "Epoch 306/1000, Loss: 2.52116190887936e+16\n",
      "Epoch 307/1000, Loss: 2.514882451944243e+16\n",
      "Epoch 308/1000, Loss: 2.5086352072638464e+16\n",
      "Epoch 309/1000, Loss: 2.502417597857792e+16\n",
      "Epoch 310/1000, Loss: 2.4962281204875264e+16\n",
      "Epoch 311/1000, Loss: 2.490066130907955e+16\n",
      "Epoch 312/1000, Loss: 2.483931199622349e+16\n",
      "Epoch 313/1000, Loss: 2.477825903611085e+16\n",
      "Epoch 314/1000, Loss: 2.4717519608610816e+16\n",
      "Epoch 315/1000, Loss: 2.4657027141730304e+16\n",
      "Epoch 316/1000, Loss: 2.4596822437658624e+16\n",
      "Epoch 317/1000, Loss: 2.4536886169042944e+16\n",
      "Epoch 318/1000, Loss: 2.44772333682688e+16\n",
      "Epoch 319/1000, Loss: 2.4417849002950656e+16\n",
      "Epoch 320/1000, Loss: 2.4358718040702976e+16\n",
      "Epoch 321/1000, Loss: 2.429986195636224e+16\n",
      "Epoch 322/1000, Loss: 2.424125498012467e+16\n",
      "Epoch 323/1000, Loss: 2.4182927176761344e+16\n",
      "Epoch 324/1000, Loss: 2.4124837744082944e+16\n",
      "Epoch 325/1000, Loss: 2.406704036918067e+16\n",
      "Epoch 326/1000, Loss: 2.4009504987283456e+16\n",
      "Epoch 327/1000, Loss: 2.395220582858752e+16\n",
      "Epoch 328/1000, Loss: 2.3895172957863936e+16\n",
      "Epoch 329/1000, Loss: 2.3838384900276224e+16\n",
      "Epoch 330/1000, Loss: 2.378185454072627e+16\n",
      "Epoch 331/1000, Loss: 2.372556899431219e+16\n",
      "Epoch 332/1000, Loss: 2.3669538998452224e+16\n",
      "Epoch 333/1000, Loss: 2.3613747373277184e+16\n",
      "Epoch 334/1000, Loss: 2.355817479143424e+16\n",
      "Epoch 335/1000, Loss: 2.350283413782528e+16\n",
      "Epoch 336/1000, Loss: 2.344767387284275e+16\n",
      "Epoch 337/1000, Loss: 2.339271117635584e+16\n",
      "Epoch 338/1000, Loss: 2.333798255558656e+16\n",
      "Epoch 339/1000, Loss: 2.3283432175960064e+16\n",
      "Epoch 340/1000, Loss: 2.322910513463296e+16\n",
      "Epoch 341/1000, Loss: 2.317498854670336e+16\n",
      "Epoch 342/1000, Loss: 2.3121069527269376e+16\n",
      "Epoch 343/1000, Loss: 2.306734378136371e+16\n",
      "Epoch 344/1000, Loss: 2.301379842408448e+16\n",
      "Epoch 345/1000, Loss: 2.2960469962653696e+16\n",
      "Epoch 346/1000, Loss: 2.290736054455501e+16\n",
      "Epoch 347/1000, Loss: 2.285444225250099e+16\n",
      "Epoch 348/1000, Loss: 2.280173441384448e+16\n",
      "Epoch 349/1000, Loss: 2.274924776600371e+16\n",
      "Epoch 350/1000, Loss: 2.2696986603945984e+16\n",
      "Epoch 351/1000, Loss: 2.2644910125481984e+16\n",
      "Epoch 352/1000, Loss: 2.2592992560807936e+16\n",
      "Epoch 353/1000, Loss: 2.2541261827211264e+16\n",
      "Epoch 354/1000, Loss: 2.2489711482241024e+16\n",
      "Epoch 355/1000, Loss: 2.243836944318464e+16\n",
      "Epoch 356/1000, Loss: 2.238720349778739e+16\n",
      "Epoch 357/1000, Loss: 2.233621149856563e+16\n",
      "Epoch 358/1000, Loss: 2.228538270810112e+16\n",
      "Epoch 359/1000, Loss: 2.223473430626304e+16\n",
      "Epoch 360/1000, Loss: 2.2184283472920576e+16\n",
      "Epoch 361/1000, Loss: 2.213402806059008e+16\n",
      "Epoch 362/1000, Loss: 2.2083955184369664e+16\n",
      "Epoch 363/1000, Loss: 2.203403263200461e+16\n",
      "Epoch 364/1000, Loss: 2.1984288320782336e+16\n",
      "Epoch 365/1000, Loss: 2.1934692185931776e+16\n",
      "Epoch 366/1000, Loss: 2.1885267849773056e+16\n",
      "Epoch 367/1000, Loss: 2.18359895425024e+16\n",
      "Epoch 368/1000, Loss: 2.1786904508760064e+16\n",
      "Epoch 369/1000, Loss: 2.173800201112781e+16\n",
      "Epoch 370/1000, Loss: 2.168925413231821e+16\n",
      "Epoch 371/1000, Loss: 2.1640650134913024e+16\n",
      "Epoch 372/1000, Loss: 2.159224800097075e+16\n",
      "Epoch 373/1000, Loss: 2.15439854534656e+16\n",
      "Epoch 374/1000, Loss: 2.149584101756109e+16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 375/1000, Loss: 2.1447846905511936e+16\n",
      "Epoch 376/1000, Loss: 2.1399992379899904e+16\n",
      "Epoch 377/1000, Loss: 2.135227958820864e+16\n",
      "Epoch 378/1000, Loss: 2.1304706382954496e+16\n",
      "Epoch 379/1000, Loss: 2.125725128930099e+16\n",
      "Epoch 380/1000, Loss: 2.1209929339633664e+16\n",
      "Epoch 381/1000, Loss: 2.1162757713821696e+16\n",
      "Epoch 382/1000, Loss: 2.111573426438144e+16\n",
      "Epoch 383/1000, Loss: 2.10688546963456e+16\n",
      "Epoch 384/1000, Loss: 2.1022108272295936e+16\n",
      "Epoch 385/1000, Loss: 2.097549069726515e+16\n",
      "Epoch 386/1000, Loss: 2.092899767628595e+16\n",
      "Epoch 387/1000, Loss: 2.088263565180928e+16\n",
      "Epoch 388/1000, Loss: 2.083639818138419e+16\n",
      "Epoch 389/1000, Loss: 2.079029600242893e+16\n",
      "Epoch 390/1000, Loss: 2.074431837752525e+16\n",
      "Epoch 391/1000, Loss: 2.06984674541568e+16\n",
      "Epoch 392/1000, Loss: 2.065274752729088e+16\n",
      "Epoch 393/1000, Loss: 2.0607128532156416e+16\n",
      "Epoch 394/1000, Loss: 2.0561614763720704e+16\n",
      "Epoch 395/1000, Loss: 2.051620836946739e+16\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6704/372101860.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstyle_targets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontent_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerated_image\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_interval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6704/3279215095.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(style_targets, content_target, generated_image, epochs, save_interval)\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerated_image\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstyle_targets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontent_target\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerated_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerated_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1082\u001b[0m                           for x in nest.flatten(output_gradients)]\n\u001b[0;32m   1083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1084\u001b[1;33m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[0;32m   1085\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1086\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[0;32m     72\u001b[0m       \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[0;32m    157\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\tensorflow\\python\\ops\\nn_grad.py\u001b[0m in \u001b[0;36m_Conv2DGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m    579\u001b[0m   \u001b[1;31m# in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m   return [\n\u001b[1;32m--> 581\u001b[1;33m       gen_nn_ops.conv2d_backprop_input(\n\u001b[0m\u001b[0;32m    582\u001b[0m           \u001b[0mshape_0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m           \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_backprop_input\u001b[1;34m(input_sizes, filter, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[0;32m   1237\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m   1240\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Conv2DBackpropInput\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_sizes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_backprop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m         \u001b[1;34m\"strides\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"use_cudnn_on_gpu\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"padding\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train(style_targets, content_target, generated_image, epochs, save_interval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-TF2.0",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
